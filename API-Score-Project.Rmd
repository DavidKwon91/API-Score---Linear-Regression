---
title: "API Score Project"
author: "David (Yongbock) Kwon"
output:
  html_document: 
    keep_md: yes
editor_options: 
  chunk_output_type: console
---

```{r}

#Predicting Academic Performance Index (API) Score 
#from California Department of Eduaction 
#through Linear Regression

library(car) #cross validation
library(ggplot2) #ggplot
library(lattice)
library(MASS)
library(dplyr)
library(caret) #linear
library(leaps) #regsubsets
library(glmnet)
library(olsrr)
library(e1071)
library(sandwich) 
library(tidyr)
library(purrr) #keep
library(ModelMetrics) #mse
library(corrplot)


set.seed(1234)

Base05<- read.csv("/Users/DavidKwon/Desktop/Yongbock/API Score Prj/API 2005 Base Data.csv")



#Basic summary of the dataset from the website, California Department of Education

#"The 2005 API (Academic Performance Index) summarizes a school's, an LEA's (local educational agency, is a school district or county office of education), or the state's performance on the spring 2005 Standardized Testing and Reporting (STAR) Program and 2005 California High School Exit Examination (CAHSEE)."

#"The 2005 API Base summarizes a subgroup's (e.g STAR Program scores, Ethnic/racial subgroup/ parents' degrees/ English Learners ...) performance on the spring 2005 STAR Program and 2005 CAHSEE. It serves as the baseline score, or starting point, of performance of that subgroup"

#Therefore, we are going to remove those variables that are created by our dependent variable such as SIM_RANK, ST_RANK
#I am also going to remove one of the variable, RTYPE or STYPE because they are overlapping each other. 

Base05<-select(Base05,c(-"RTYPE", -"SIM_RANK",-"ST_RANK"))

#Finding NAs rows of the dependent variables and removes the rows that includes NAs

Base05<-Base05[-which(is.na(Base05$API05B)),] 

#Creating function that finds NAs for each variables 

findingNA<-function(x){
  length(which(is.na(x)))/length(x)
}


for(i in 1:dim(Base05)[2]){
  if(findingNA(Base05[,i])>0.1){
    print(i)
  }
}


#I am going to remove the variables that have 10% of the elements is NAs 
#I also removed the "X" variable (school or district code), which is 1st column

newbase<-Base05[,c(-1, -3, -4, -5, -42, -46, -47, -48, -66)]

new.base<-newbase[complete.cases(newbase),]

```

```{r}
#lets explore our dependent variable with ggplot

#Histogram of our dependent variable

ggplot(data=new.base, aes(new.base$API05B)) + 
  geom_histogram(aes(y=..density..), binwidth=10) +
  geom_density(aes(y=..density..), color="red")+
  labs(title="Histogram for Academic Performance Index 2005 Base", x="API 2005 Base", y="Frequency") +
  geom_vline(xintercept = mean(new.base$API05B), show.legend = TRUE, color="red") +
  geom_vline(xintercept = median(new.base$API05B), show.legend = TRUE, color="blue")


#As the graph shows, we can notice the API05B seems to be left-skewed. 
```

```{r}
###Our independent factor variables

new.base[1:97] %>% keep(is.factor) %>% gather() %>%
  ggplot(aes(value))+
  facet_wrap(~key,scales="free")+
  geom_bar()

```

```{r}
#Our independent numerical variable 

new.base[1:20] %>% keep(is.numeric) %>% gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales="free")+
  geom_histogram(bins=30)+
  geom_density()

new.base[21:40] %>% keep(is.numeric) %>% gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales="free")+
  geom_histogram(bins=30)+
  geom_density()

new.base[41:60] %>% keep(is.numeric) %>% gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales="free")+
  geom_histogram(bins=30)+
  geom_density()

new.base[61:80] %>% keep(is.numeric) %>% gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales="free")+
  geom_histogram(bins=30)+
  geom_density()

new.base[81:96] %>% keep(is.numeric) %>% gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales="free")+
  geom_histogram(bins=30)+
  geom_density()


#Most of our independent numerical variables are highly skewed
#We might want to perform log tranformation, but I would like to see the model before transformation, and if transformation is needed, I will perform it then. 
```

```{r}
#Splitting training and test dataset - cross validation

split<-createDataPartition(y=new.base$API05B,p=0.7,list=FALSE)

training.newbase <- new.base[split,]
test.newbase <- new.base[-split,]
```

```{r}
#The first model with whole data

lm<-lm(API05B~., data=training.newbase)
summary(lm)

plot(lm)


#There's a lot of NAs of Beta, which implies that it's not estimatable. 
#We are going to perform variable selection using regsubsets-backward
```

```{r}
###regsubsets - model selection - backward - nvmax=10

#Model Selection through stepwise backward selection from regsubsets

#I let the maximum number of predictors be 10, since it's going to be too complex to interpret or to build a model if we have more than 10 predictors

reg1<-regsubsets(API05B~., data=training.newbase, really.big=TRUE,nvmax=10, method = "backward")

reg.summary<-summary(reg1)

reg.summary


#I am going to see the changes of R^2, Adjusted R^2 (how well the model explained), Residuals Sum of Sqaured (Residuals), Cp (size of the bias), BIC 

###R Squared

plot(reg.summary$rsq, xlab="number of variables", ylab="Rsquared", type="l")
max(reg.summary$rsq)
which.max(reg.summary$rsq)
points(3, reg.summary$rsq[3], col="red",cex=1,pch=20)
points(4, reg.summary$rsq[4], col="red",cex=1,pch=20)
```

```{r}
#As the graph shows, we can notice a significant change of R^2 at 3 and 4 of number of variables
#After that, we see the graph is slightly increasing 
#I assume that those points are the critical points.  


###Residuals Sum of Squared

plot(reg.summary$rss, xlab="number of variables", ylab="RSS", type="l")
min(reg.summary$rss)
which.min(reg.summary$rss)
points(3, reg.summary$rss[3], col="red",cex=1,pch=20)
points(4, reg.summary$rss[4], col="red",cex=1,pch=20)

```

```{r}
###Adjusted R Squared

plot(reg.summary$adjr2, xlab="number of variables", ylab="Adjusted R^2", type="l")
max(reg.summary$adjr2)
which.max(reg.summary$adjr2)
points(3,reg.summary$adjr2[3],col="red",cex=1,pch=20)
points(4,reg.summary$adjr2[4],col="red",cex=1,pch=20)

```

```{r}
###CP
#From Penn State Univ Stat online Course..
#(https://onlinecourses.science.psu.edu/stat501/node/330/)

#"An underspecified model is a model in which important predictors are missing. And, an underspecified model yields biased regression coefficients and biased predictions of the response. Well, in short, Mallows' CP statistic estimates the size of the bias that is introduced into the predicted responses by having an underspecified model."
#In short, CP implies the size of the bias

plot(reg.summary$cp, xlab="number of variables", ylab="Cp", type="l")
min(reg.summary$cp)
which.min(reg.summary$cp)
points(3, reg.summary$cp[3],col="red",cex=1,pch=20)
points(4, reg.summary$cp[4],col="red",cex=1,pch=20)
```

```{r}
###BIC

plot(reg.summary$bic, xlab="number of variables", ylab="BIC", type="l")
min(reg.summary$bic)
which.min(reg.summary$bic)
points(3, reg.summary$bic[3],col="red",cex=1,pch=20)
points(4, reg.summary$bic[4],col="red",cex=1,pch=20)
```

```{r}
#I would try to choose 4 number of predictors for our model from regsubsets. 

a<-c("MEALS", "SMOB","GRAD_SCH","CW_CSTE")

reg.summary$rsq[4]
reg.summary$adjr2[4]


training.newbase1<-select(training.newbase,c("API05B",a))
rownames(training.newbase1)<-1:nrow(training.newbase1)
```

```{r}
#Plots between dependent variable and independent variables
plot(training.newbase1)
```

```{r}
#GRAD_SCH with others shows some curves, it looks like log or squared term's graph

training.newbase1 %>% keep(is.numeric) %>% gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales="free")+
  geom_histogram(bins=30)+
  geom_density()

```

```{r}
lm1<-lm(API05B~., data=training.newbase1)
summary(lm1)

plot(lm1)
```

```{r}
#####Adj R^2 75%, and there looks some patterns and some outliers 

a<-predict(lm1,newdata=test.newbase)
b<-test.newbase$API05B


plot(a,b)
abline(0,1,col="red")
```

```{r}
#Predicted values vs test values of API05B
#The graph looks like linear, and some outliers
predictions1 <- lm1 %>% predict(test.newbase)
```

```{r}
###Model performance
data.frame(
  MSE = mse(test.newbase$API05B, predictions1),
  RMSE = RMSE(predictions1, test.newbase$API05B),
  R2 = R2(predictions1, test.newbase$API05B)
)

```

```{r}
###1. Outliers

#Finding outliers in our model

#There are several ways to see outliers in our model

outlierTest(lm1,n.max=50,cutoff=0.05)
```
```{r}
#outlierTest shows which observations have greater than absolute value of 4 of rstudent. 


qqPlot(lm1,main="QQ Plot")
```

```{r}
#qqPlot shows which observations have an impact on normality of residuals - which is outliers or bad leverage points


influencePlot(lm1,id.method="identify",main="influential plot",sub="circle size is proportial to cook's distance")
```

```{r}
#Hat Values against Studentized Residuals

#However,
#I'm using Standardized Residuals to detect outliers

#From Springers Text book "Linear Regression", Chapter 3.2 - Regression Diagnostics: Tools for Checking the Validity of a Model, pg 60. 

#"In summary, an outlier is a point whose standardized residual falls outside the interval from -2 to 2. Recall that a bad leverage point is a leverage point which is also an outlier. Thus, a bad leverage point is a leverage point whose standardized residual falls outside the interval from -2 to 2."
####In very large datasets, we apply the rule to -4 to 4. 

o1<-which(rstandard(lm1, infl = lm.influence(lm1, do.coef = FALSE),
                    sd=sqrt(deviance(lm1)/df.residual(lm1)),
                    type=c("sd.1","predictive"))>4)

o2<-which(rstandard(lm1, infl = lm.influence(lm1, do.coef = FALSE),
                    sd=sqrt(deviance(lm1)/df.residual(lm1)),
                    type=c("sd.1","predictive"))<(-4))


outliers <- c(o1,o2)
length(outliers)
```

```{r}
#The data set after removing outliers
training.newbase2<-training.newbase1[-outliers,]
```

```{r}
#Investigating linear models between response and each predictors - partial regression
# API05B = B0 + B1*MEALS + E

lm.test1<-lm(API05B~MEALS, data=training.newbase2)
summary(lm.test1)

plot(lm.test1)
```

```{r}
#Adj R^2 29.6%, plots looks ok except for normal Q-Q plot

# API05B = B0 + B1*SMOB + E

lm.test2<-lm(API05B~SMOB, data=training.newbase2)
summary(lm.test2)
plot(lm.test2)

```

```{r}
#Adj R^2 40%, seems heteroscedasticity

# API05B = B0 + B1*GRAD_SCH + E

lm.test3<-lm(API05B~GRAD_SCH, data=training.newbase2)
summary(lm.test3)
plot(lm.test3)
```


```{r}
#R^2 39.5%, plots shows extreme heteroscedasticity

# API05B = B0 + B1*CW_CSTE + E

lm.test4<-lm(API05B~CW_CSTE, data=training.newbase2)
summary(lm.test4)
plot(lm.test4)
```

```{r}
#R^2 21.8%, plots shows some patterns showing heteroscedasticity

###Model after removing outliers

lm2<-lm(API05B~., data=training.newbase2)
summary(lm2)
plot(lm2)
```


```{r}
#Adj R^2 77%, plots looks ok except for showing some pattern of the heteroscedasticity. 

#R^2 getting higher, plots for model such as 'fitted values vs residuals', 'Normal Q-Q', or 'sqrt of Standardized residuals vs fitted values' becomes much better after removing outliers

#It's still showing some patterns, I will perform diagnosis of normality of residuals, multicollinearity, and heteroscedasticity from now. 

a1<-predict(lm2,newdata = test.newbase)

predictions2 <- lm2 %>% predict(test.newbase)
```

```{r}
### Model performance

data.frame(
  MSE = mse(test.newbase$API05B, predictions2),
  RMSE = RMSE(predictions2, test.newbase$API05B),
  R2 = R2(predictions2, test.newbase$API05B)
)

```

```{r}
###2. Normality (of residuals)

#We don't have to worry about the assumption of normality of residuals (the error between the dependent variable and the independent variables) because when the sample size is sufficiently large, the Central Limit Theorem ensures that the distribution of residiual will be approximately normality. 


qplot(residuals(lm2),
      geom="histogram",
      binwidth= 10,
      main="Histogram of Residuals of our model",
      xlab="Residuals of our model",
      ylab="Frequency")
mean(residuals(lm2))
```


```{r}
#Residuals on our model are approximately normal distributed with zero mean. 



###3. Multicolliniearity

#From STHDA website.. 

#definition

#"In multiple regression , two or more predictor variables might be correlated with each other. This situation is referred as collinearity.

#There is an extreme situation, called multicollinearity, where collinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This means that there is redundancy between predictor variables."

cor<-cor(training.newbase2)
corrplot(cor, 
         method=c("circle"),
         title="Correlation between variables",
         type=c("full"))

vif(lm2)
```


```{r}
####More than 4 VIF score must be removed, but we dont have any.

###4. Heteroscedasciticity (non-constant residuals)


plot(lm2)
```


```{r}
#As we see the first plot of the model (fitted value vs residuals), 
#the residuals plots show decreasing variance towards the right end.
#It seems like there's heteroscedasticity exists, so I'm going to perform transformation. 

###Box-cox
bc<-boxcox(lm2)
```


```{r}
#Finding maximum log-likelihoods for the parameter of the Box-Cox power trasnformation, which is lambda

lambda<-bc$x[which.max(bc$y)]
lambda

```

```{r}
###Boxcox power + log transformation

lm3<-lm(API05B^(lambda)~MEALS+log(SMOB+1)+log(GRAD_SCH+1)+CW_CSTE, data=training.newbase2)
summary(lm3)

plot(lm3)
```

```{r}
#Adj R^2 76.8%. 
#The residuals plots look improved, which is that it looks like now it's homoscedasticity (constant residuals, no patterns)

a2<-sqrt(predict(lm3,newdata = test.newbase))
```

```{r}
###Model Progress by showing graphs of "predicted vs test values"

plot(a,b,
     main="First model prediction",
     xlab="predicted values",
     ylab="values in test dataset")
abline(a=0,b=1,col="red")
plot(a1,b,
     main="Removed outliers",
     xlab="predicted values",
     ylab="values in test dataset")
abline(a=0,b=1,col="red")
plot(a2,b,
     main="After transformation - Final model",
     xlab="predicted values",
     ylab="values in test dataset")
abline(a=0,b=1,col="red")

```


```{r}
#The graphs with final model seems to be linear, and there is no patters. 

predictions3 <- lm3 %>% predict(test.newbase)
```

```{r}
### Model performance

data.frame(
  MSE = mse(test.newbase$API05B, predictions3),
  RMSE = RMSE(predictions3, test.newbase$API05B),
  R2 = R2(predictions3, test.newbase$API05B)
)
```

```{r}
###Model performance changing process

c.m<-data.frame(MSE=
                  c(mse(test.newbase$API05B, predictions1),
                    mse(test.newbase$API05B, predictions2),
                    mse(test.newbase$API05B, predictions3)
                  ),
                RMSE=
                  c(RMSE(predictions1,test.newbase$API05B),
                    RMSE(predictions2,test.newbase$API05B),
                    RMSE(predictions3,test.newbase$API05B)
                  ),
                R2=
                  c(R2(predictions1,test.newbase$API05B),
                    R2(predictions2,test.newbase$API05B),
                    R2(predictions3,test.newbase$API05B)
                  ))


rownames(c.m)<-c("First model","Removed Outliers","After transformation-final model")

c.m

summary(lm3)
```

```{r}
#Conclusion
  
#I conclude that we are able to predict the API score for the future with the 3 of predictors, which are described at below, as 76.8% variance explained with all transformed predictors have less than 0.05 p-values, which is that they are all significant predictors on our response variable. 
  
#As interpretation of the Betas (each coefficients),    
  
#For example,  
  
   
   
#- For a 1 units increase in MEALS, abs(Beta1) = 2127.65 decrease in (API05B) ^ (lambda), where lambda is 2


#- For a 1% increase in SMOB, abs(Beta2) * log(1.01+1) = 61332.05 * log(1.01+1) = 42818.66
  

#Therefore, abs(Beta2) * log(1.01+1) = 61332.05 * log(1.01+1) = 42818.66 decrease in squared API05B score
  
#In other words, 1 percent change in SMOB is associated with abs(Beta2) * log(1.01+1) change in (API05B)^(lambda), where lambda is 2

#- For a 10% increase in GRAD_SCH, Beta3 * log(1.1+1) = 35881.42 * log(1.1+1) = 26621.77

#Therefore, Beta3 * log(1.1+1) = 35881.42 * log(1.1+1) = 26621.77 increase in squared API05B score

#In other words, 1 percent change in GRAD_SCH is associated with Beta3 * log(1.1+1) change in (API05B)^(lambda), where lambda is 2 

#- For a 10 units increase in CW_CSTE, Beta4 * 10 = 6170.79 * 10 = 61707.9 increase in squared API05B score
